{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_second_underscore(s):\n",
    "    index_to_replace = s.find('_', s.find('_') + 1)\n",
    "    s = s[:index_to_replace] + '/' + s[index_to_replace + 1:]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 128, 128])\n",
      "tensor([2, 1, 4, 0, 6, 3, 4, 1, 4, 2, 4, 5, 3, 4, 6, 4, 5, 6, 6, 6, 1, 1, 1, 0,\n",
      "        5, 6, 4, 1, 3, 6, 5, 0])\n",
      "tensor([[-0.1000,  0.0000],\n",
      "        [ 0.4000,  0.3000],\n",
      "        [-0.4000,  0.4000],\n",
      "        [ 0.0000,  0.2000],\n",
      "        [-0.4000,  0.4000],\n",
      "        [ 0.0000,  0.5000],\n",
      "        [-0.4000,  0.2000],\n",
      "        [ 0.4000,  0.3000],\n",
      "        [-0.5000,  0.4000],\n",
      "        [-0.2000, -0.2000],\n",
      "        [-0.5000,  0.1000],\n",
      "        [-0.4000,  0.2000],\n",
      "        [-0.3000,  0.8000],\n",
      "        [-0.3000,  0.5000],\n",
      "        [-0.7000,  0.6000],\n",
      "        [-0.6000,  0.2000],\n",
      "        [-0.6000,  0.3000],\n",
      "        [-0.5000,  0.8000],\n",
      "        [-0.6000,  0.4000],\n",
      "        [-0.5000,  0.4000],\n",
      "        [ 0.4000,  0.3000],\n",
      "        [ 0.4000,  0.3000],\n",
      "        [ 0.4000,  0.3000],\n",
      "        [-0.1000,  0.0000],\n",
      "        [-0.6000,  0.2000],\n",
      "        [-0.5000,  0.3000],\n",
      "        [-0.5000,  0.4000],\n",
      "        [ 0.5000,  0.4000],\n",
      "        [-0.1000,  0.7000],\n",
      "        [-0.7000,  0.5000],\n",
      "        [-0.5000,  0.1000],\n",
      "        [ 0.0000, -0.1000]])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file with labels.\n",
    "            image_dir (str): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.df['image_exists'] = self.df['subDirectory_filePath'].apply(lambda x: os.path.exists(os.path.join(self.image_dir, replace_second_underscore(x))))\n",
    "        self.df = self.df[self.df['image_exists']].reset_index(drop=True)\n",
    "\n",
    "        # Label encoding for emotion classes\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.df['expression'] = self.label_encoder.fit_transform(self.df['expression'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image and label\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = row['subDirectory_filePath']\n",
    "        image_path = replace_second_underscore(image_path)\n",
    "        # index_to_replace = image_path.find('_', image_path.find('_') + 1)\n",
    "        # image_path = image_path[:index_to_replace] + '/' + image_path[index_to_replace + 1:]\n",
    "        expression = row['expression']\n",
    "        valence = row['valence']\n",
    "        arousal = row['arousal']\n",
    "        \n",
    "        # Construct the image path\n",
    "        image_path = os.path.join(self.image_dir, image_path)\n",
    "        \n",
    "        # Load the image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Return the image, emotion class label and valence-arousal\n",
    "        return image, expression, torch.tensor([valence, arousal], dtype=torch.float)\n",
    "\n",
    "# Define image transformations (resize, normalization, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for pre-trained models\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Initialize the dataset and DataLoader\n",
    "dataset = EmotionDataset(csv_file='DiffusionFER/DiffusionEmotion_S/dataset_sheet.csv', image_dir='DiffusionFER/', transform=transform)\n",
    "# # Count the number of instances for each class\n",
    "# class_counts = np.zeros(7)  # Assuming 7 emotion classes\n",
    "# for _, emotion_class, _ in dataset:\n",
    "#     class_counts[emotion_class] += 1\n",
    "\n",
    "# # Calculate weights for each class (inverse of frequency)\n",
    "# class_weights = 1.0 / class_counts\n",
    "# sample_weights = [class_weights[label] for _, label, _ in dataset]\n",
    "\n",
    "# # Create the WeightedRandomSampler\n",
    "# sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Count the number of instances per class\n",
    "class_counts = Counter([label for _, label, _ in dataset])\n",
    "max_count = max(class_counts.values())\n",
    "\n",
    "# Create an empty list to hold the upsampled data\n",
    "upsampled_data = []\n",
    "\n",
    "# Perform upsampling for each class\n",
    "for emotion_class in class_counts:\n",
    "    class_samples = [sample for sample in dataset if sample[1] == emotion_class]\n",
    "    # Duplicate until the number matches the majority class\n",
    "    while len(class_samples) < max_count:\n",
    "        # Randomly sample with replacement to balance the classes\n",
    "        class_samples.append(random.choice(class_samples))\n",
    "    \n",
    "    upsampled_data.extend(class_samples)\n",
    "\n",
    "# Shuffle the upsampled dataset to avoid ordering bias\n",
    "random.shuffle(upsampled_data)\n",
    "\n",
    "# Create a DataLoader with the upsampled data\n",
    "dataloader = DataLoader(upsampled_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create a balanced DataLoader using the sampler\n",
    "# dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# Filter out rows with non-existent images\n",
    "\n",
    "\n",
    "# Check one batch of data\n",
    "for images, expressiones, valence_arousal in dataloader:\n",
    "    print(images.shape)  # Should be (batch_size, 3, 128, 128)\n",
    "    print(expressiones)\n",
    "    print(valence_arousal)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, 7)  # Output for emotion classification (7 classes)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 2)  # Output for valence-arousal regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        \n",
    "        emotion_class_output = self.fc2(x)  # Emotion class prediction\n",
    "        valence_arousal_output = self.fc3(x)  # Valence-arousal prediction\n",
    "        \n",
    "        return emotion_class_output, valence_arousal_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmotionModel()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for multi-task learning\n",
    "criterion_classification = nn.CrossEntropyLoss()\n",
    "criterion_regression = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the dataset into training and testing sets\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# test_size = len(dataset) - train_size\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "# # Create DataLoader for training and testing sets\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/7], Loss: 1.9464, Reg Loss: 0.1442, Acc: 15.54%\n",
      "Epoch [2/7], Loss: 1.9464, Reg Loss: 0.1443, Acc: 15.54%\n",
      "Epoch [3/7], Loss: 1.9464, Reg Loss: 0.1442, Acc: 15.54%\n",
      "Epoch [4/7], Loss: 1.9464, Reg Loss: 0.1442, Acc: 15.54%\n",
      "Epoch [5/7], Loss: 1.9464, Reg Loss: 0.1442, Acc: 15.54%\n",
      "Epoch [6/7], Loss: 1.9464, Reg Loss: 0.1442, Acc: 15.54%\n",
      "Epoch [7/7], Loss: 1.9464, Reg Loss: 0.1442, Acc: 15.54%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 7\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss_classification = 0.0\n",
    "    running_loss_regression = 0.0\n",
    "    correct_classifications = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for images, emotion_classes, valence_arousal in dataloader:\n",
    "        # Move to device (if using GPU)\n",
    "        images = images.to(device)\n",
    "        emotion_classes = emotion_classes.to(device)\n",
    "        valence_arousal = valence_arousal.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        # emotion_class_output = model(images)\n",
    "        emotion_class_output, valence_arousal_output = model(images)\n",
    "        \n",
    "        # Calculate the classification loss (cross-entropy)\n",
    "        loss_classification = criterion_classification(emotion_class_output, emotion_classes)\n",
    "        \n",
    "        # Calculate the regression loss (MSE)\n",
    "        loss_regression = criterion_regression(valence_arousal_output, valence_arousal)\n",
    "        \n",
    "        # Total loss is a sum of both losses\n",
    "        # total_loss = loss_classification\n",
    "        total_loss = loss_classification + loss_regression\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss_classification += loss_classification.item()\n",
    "        running_loss_regression += loss_regression.item()\n",
    "        \n",
    "        # Accuracy for classification\n",
    "        _, predicted = torch.max(emotion_class_output, 1)\n",
    "        correct_classifications += (predicted == emotion_classes).sum().item()\n",
    "        total_samples += images.size(0)\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    # Print epoch statistics\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss_classification/len(dataloader):.4f}, '\n",
    "          f'Reg Loss: {running_loss_regression/len(dataloader):.4f}, '\n",
    "          f'Acc: {100 * correct_classifications/total_samples:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'emotion_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
